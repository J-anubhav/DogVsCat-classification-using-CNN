{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d11369a",
   "metadata": {},
   "source": [
    "# New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845eaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, \n",
    "                                   Dropout, BatchNormalization, GlobalAveragePooling2D)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263eb4b",
   "metadata": {},
   "source": [
    "## Load and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dbebdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load data with improved preprocessing\"\"\"\n",
    "    try:\n",
    "        X_train = np.loadtxt('input.csv', delimiter=',')\n",
    "        X_test = np.loadtxt('input_test.csv', delimiter=',')\n",
    "        Y_train = np.loadtxt('labels.csv', delimiter=',')\n",
    "        Y_test = np.loadtxt('labels_test.csv', delimiter=',')\n",
    "    except ValueError:\n",
    "        import pandas as pd\n",
    "        X_train = pd.read_csv('input.csv').to_numpy()\n",
    "        X_test = pd.read_csv('input_test.csv').to_numpy()\n",
    "        Y_train = pd.read_csv('labels.csv').to_numpy()\n",
    "        Y_test = pd.read_csv('labels_test.csv').to_numpy()\n",
    "    \n",
    "    # Reshape data\n",
    "    X_train = X_train.reshape(-1, 100, 100, 3)\n",
    "    X_test = X_test.reshape(-1, 100, 100, 3)\n",
    "    Y_train = Y_train.reshape(-1, 1)\n",
    "    Y_test = Y_test.reshape(-1, 1)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Per-channel normalization \n",
    "    # Compute mean and std per channel on training data\n",
    "    train_mean = np.mean(X_train, axis=(0, 1, 2))\n",
    "    train_std = np.std(X_train, axis=(0, 1, 2))\n",
    "    \n",
    "    # Apply normalization\n",
    "    X_train = (X_train - train_mean) / (train_std + 1e-7)\n",
    "    X_test = (X_test - train_mean) / (train_std + 1e-7)\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Training labels distribution: {np.bincount(Y_train.astype(int).flatten())}\")\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d8e8e",
   "metadata": {},
   "source": [
    "# data augmentatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b6d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generators(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Create data generators with more aggressive augmentation\"\"\"\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.3,\n",
    "        shear_range=0.2,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # No augmentation for validation data\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    train_generator = train_datagen.flow(X_train, Y_train, batch_size=32)\n",
    "    val_generator = val_datagen.flow(X_test, Y_test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_generator, val_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472e3f9",
   "metadata": {},
   "source": [
    "# CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89fbc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_model():\n",
    "    \"\"\"Create an improved CNN model with better architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fourth Convolutional Block\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Global Average Pooling instead of Flatten\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba439f73",
   "metadata": {},
   "source": [
    "# Transfer learning model (VGG16-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "608f81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_model():\n",
    "    \"\"\"Create a transfer learning model using VGG16\"\"\"\n",
    "    from tensorflow.keras.applications import VGG16\n",
    "    from tensorflow.keras.models import Model\n",
    "    \n",
    "    # Load pre-trained VGG16 model\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom classifier\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208d97c",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b748c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, val_generator, model_name=\"enhanced_model\"):\n",
    "    \"\"\"Train the model with advanced callbacks\"\"\"\n",
    "    \n",
    "    # Compile with optimized parameters\n",
    "    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Advanced callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'{model_name}_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,  # More epochs with early stopping\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bce0c3",
   "metadata": {},
   "source": [
    "# Evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2362f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize(model, X_test, Y_test):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred.flatten() == Y_test.flatten())\n",
    "    print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(Y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Dog', 'Cat'], yticklabels=['Dog', 'Cat'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Enhanced Model')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(Y_test, y_pred, target_names=['Dog', 'Cat']))\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581508c7",
   "metadata": {},
   "source": [
    "# Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10866da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_prediction(models, X_test):\n",
    "    \"\"\"Create ensemble predictions from multiple models\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = np.mean(predictions, axis=0)\n",
    "    return (ensemble_pred > 0.5).astype(int)\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"ENHANCED DOG VS CAT CLASSIFICATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Load and preprocess data\n",
    "    print(\"\\n1. Loading and preprocessing data...\")\n",
    "    X_train, X_test, Y_train, Y_test = load_and_preprocess_data()\n",
    "    \n",
    "    # Step 2: Create data generators\n",
    "    print(\"\\n2. Creating data generators...\")\n",
    "    train_gen, val_gen = create_data_generators(X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    # Step 3: Train improved CNN model\n",
    "    print(\"\\n3. Training improved CNN model...\")\n",
    "    improved_model = create_improved_model()\n",
    "    print(f\"Model parameters: {improved_model.count_params():,}\")\n",
    "    \n",
    "    history1 = train_model(improved_model, train_gen, val_gen, \"improved_cnn\")\n",
    "    acc1 = evaluate_and_visualize(improved_model, X_test, Y_test)\n",
    "    \n",
    "    # Step 4: Train transfer learning model\n",
    "    print(\"\\n4. Training transfer learning model...\")\n",
    "    transfer_model = create_transfer_learning_model()\n",
    "    print(f\"Transfer model parameters: {transfer_model.count_params():,}\")\n",
    "    \n",
    "    # Recreate generators for transfer learning\n",
    "    train_gen2, val_gen2 = create_data_generators(X_train, Y_train, X_test, Y_test)\n",
    "    history2 = train_model(transfer_model, train_gen2, val_gen2, \"transfer_learning\")\n",
    "    acc2 = evaluate_and_visualize(transfer_model, X_test, Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3857a4",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "974254d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_prediction(models, X_test):\n",
    "    \"\"\"Create ensemble predictions from multiple models\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = np.mean(predictions, axis=0)\n",
    "    return (ensemble_pred > 0.5).astype(int)\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"ENHANCED DOG VS CAT CLASSIFICATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Load and preprocess data\n",
    "    print(\"\\n1. Loading and preprocessing data...\")\n",
    "    X_train, X_test, Y_train, Y_test = load_and_preprocess_data()\n",
    "    \n",
    "    # Step 2: Create data generators\n",
    "    print(\"\\n2. Creating data generators...\")\n",
    "    train_gen, val_gen = create_data_generators(X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    # Step 3: Train improved CNN model\n",
    "    print(\"\\n3. Training improved CNN model...\")\n",
    "    improved_model = create_improved_model()\n",
    "    print(f\"Model parameters: {improved_model.count_params():,}\")\n",
    "    \n",
    "    history1 = train_model(improved_model, train_gen, val_gen, \"improved_cnn\")\n",
    "    acc1 = evaluate_and_visualize(improved_model, X_test, Y_test)\n",
    "    \n",
    "    # Step 4: Train transfer learning model\n",
    "    print(\"\\n4. Training transfer learning model...\")\n",
    "    transfer_model = create_transfer_learning_model()\n",
    "    print(f\"Transfer model parameters: {transfer_model.count_params():,}\")\n",
    "    \n",
    "    # Recreate generators for transfer learning\n",
    "    train_gen2, val_gen2 = create_data_generators(X_train, Y_train, X_test, Y_test)\n",
    "    history2 = train_model(transfer_model, train_gen2, val_gen2, \"transfer_learning\")\n",
    "    acc2 = evaluate_and_visualize(transfer_model, X_test, Y_test)\n",
    "    \n",
    "    # Step 5: Ensemble prediction\n",
    "    print(\"\\n5. Creating ensemble prediction...\")\n",
    "    ensemble_pred = create_ensemble_prediction([improved_model, transfer_model], X_test)\n",
    "    ensemble_acc = np.mean(ensemble_pred.flatten() == Y_test.flatten())\n",
    "    print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "    \n",
    "    # Final comparison\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULTS COMPARISON:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Improved CNN Accuracy: {acc1:.4f}\")\n",
    "    print(f\"Transfer Learning Accuracy: {acc2:.4f}\")\n",
    "    print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return improved_model, transfer_model, ensemble_acc\n",
    "\n",
    "# Additional utility functions\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_prediction_examples(model, X_test, Y_test, num_examples=8):\n",
    "    \"\"\"Show prediction examples\"\"\"\n",
    "    indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        \n",
    "        # Denormalize image for display (if normalized)\n",
    "        img = X_test[idx]\n",
    "        if img.min() < 0:  # If normalized\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        \n",
    "        pred_prob = model.predict(np.expand_dims(X_test[idx], axis=0))[0][0]\n",
    "        pred_class = \"Cat\" if pred_prob > 0.5 else \"Dog\"\n",
    "        true_class = \"Cat\" if Y_test[idx][0] > 0.5 else \"Dog\"\n",
    "        \n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        plt.title(f'True: {true_class}\\nPred: {pred_class} ({pred_prob:.2f})', \n",
    "                 color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69ea25",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84d1b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ENHANCED DOG VS CAT CLASSIFICATION\n",
      "==================================================\n",
      "\n",
      "1. Loading and preprocessing data...\n",
      "Training data shape: (2000, 100, 100, 3)\n",
      "Test data shape: (400, 100, 100, 3)\n",
      "Training labels distribution: [1000 1000]\n",
      "\n",
      "2. Creating data generators...\n",
      "\n",
      "3. Training improved CNN model...\n",
      "Model parameters: 849,313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "d:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.5031 - loss: 1.1193\n",
      "Epoch 1: val_accuracy improved from None to 0.50000, saving model to improved_cnn_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 247ms/step - accuracy: 0.5215 - loss: 1.0778 - val_accuracy: 0.5000 - val_loss: 0.8777 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5401 - loss: 1.0289\n",
      "Epoch 2: val_accuracy improved from 0.50000 to 0.51250, saving model to improved_cnn_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 244ms/step - accuracy: 0.5295 - loss: 1.0321 - val_accuracy: 0.5125 - val_loss: 0.7163 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.5320 - loss: 0.9189\n",
      "Epoch 3: val_accuracy did not improve from 0.51250\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 230ms/step - accuracy: 0.5485 - loss: 0.9241 - val_accuracy: 0.5050 - val_loss: 0.7891 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.5471 - loss: 0.8873\n",
      "Epoch 4: val_accuracy improved from 0.51250 to 0.53250, saving model to improved_cnn_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 242ms/step - accuracy: 0.5375 - loss: 0.8883 - val_accuracy: 0.5325 - val_loss: 0.6815 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.5804 - loss: 0.7711\n",
      "Epoch 5: val_accuracy improved from 0.53250 to 0.58500, saving model to improved_cnn_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.5580 - loss: 0.8012 - val_accuracy: 0.5850 - val_loss: 0.7049 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.5598 - loss: 0.7739\n",
      "Epoch 6: val_accuracy did not improve from 0.58500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 240ms/step - accuracy: 0.5680 - loss: 0.7673 - val_accuracy: 0.5800 - val_loss: 0.6615 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m26/63\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 224ms/step - accuracy: 0.6032 - loss: 0.7160"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     improved_model, transfer_model, final_accuracy = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m improved_model = create_improved_model()\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimproved_model.count_params()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m history1 = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimproved_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimproved_cnn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m acc1 = evaluate_and_visualize(improved_model, X_test, Y_test)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Step 4: Train transfer learning model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_generator, val_generator, model_name)\u001b[39m\n\u001b[32m     13\u001b[39m callbacks = [\n\u001b[32m     14\u001b[39m     EarlyStopping(\n\u001b[32m     15\u001b[39m         monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     )\n\u001b[32m     33\u001b[39m ]\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More epochs with early stopping\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\DogVsCat-classification-using-CNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    improved_model, transfer_model, final_accuracy = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e582c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b62437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_examples(model, X_test, Y_test, model_name=\"Model\", num_examples=8):\n",
    "    \"\"\"Show prediction examples with images, true labels, and predicted labels\"\"\"\n",
    "    \n",
    "    # Select random examples\n",
    "    indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "    \n",
    "    # Create subplot\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        \n",
    "        # Get the image\n",
    "        img = X_test[idx].copy()\n",
    "        \n",
    "        # Denormalize image for display (since we normalized it during preprocessing)\n",
    "        # Reverse the per-channel normalization if you applied it\n",
    "        if img.min() < 0:  # If the image was normalized with mean/std\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "        else:\n",
    "            img = np.clip(img, 0, 1)  # Ensure values are in [0,1] range\n",
    "        \n",
    "        # Display image\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # Make prediction\n",
    "        pred_prob = model.predict(np.expand_dims(X_test[idx], axis=0), verbose=0)[0][0]\n",
    "        pred_class = \"Cat\" if pred_prob > 0.5 else \"Dog\"\n",
    "        true_class = \"Cat\" if Y_test[idx][0] > 0.5 else \"Dog\"\n",
    "        \n",
    "        # Set title color based on correctness\n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        \n",
    "        # Create title with prediction confidence\n",
    "        title = f'True: {true_class}\\nPred: {pred_class}\\nConf: {pred_prob:.3f}'\n",
    "        plt.title(title, color=color, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Remove axis\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Prediction Examples\\n(Green=Correct, Red=Incorrect)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_single_prediction(model, X_test, Y_test, model_name=\"Model\"):\n",
    "    \"\"\"Show a single random prediction - similar to your original code\"\"\"\n",
    "    \n",
    "    # Pick a random image\n",
    "    idx = random.randint(0, len(Y_test) - 1)\n",
    "    \n",
    "    # Display the image\n",
    "    img = X_test[idx].copy()\n",
    "    if img.min() < 0:  # Denormalize if needed\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Get true label\n",
    "    true_label = \"Cat\" if Y_test[idx, 0] > 0.5 else \"Dog\"\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_prob = model.predict(np.expand_dims(X_test[idx], axis=0), verbose=0)\n",
    "    pred_class = \"Cat\" if pred_prob > 0.5 else \"Dog\"\n",
    "    \n",
    "    # Set title\n",
    "    plt.title(f\"True label: {true_label}\", fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print prediction\n",
    "    print(f\"Prediction: {pred_class}\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Confidence: {pred_prob[0][0]:.4f}\")\n",
    "    if pred_class == true_label:\n",
    "        print(\"✅ CORRECT PREDICTION!\")\n",
    "    else:\n",
    "        print(\"❌ INCORRECT PREDICTION!\")\n",
    "\n",
    "def compare_model_predictions(models, model_names, X_test, Y_test, num_examples=4):\n",
    "    \"\"\"Compare predictions from multiple models side by side\"\"\"\n",
    "    \n",
    "    indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, len(models) + 1, figsize=(20, 5 * num_examples))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Show original image\n",
    "        img = X_test[idx].copy()\n",
    "        if img.min() < 0:\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        axes[i, 0].imshow(img)\n",
    "        true_class = \"Cat\" if Y_test[idx][0] > 0.5 else \"Dog\"\n",
    "        axes[i, 0].set_title(f'True: {true_class}', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Show predictions from each model\n",
    "        for j, (model, name) in enumerate(zip(models, model_names)):\n",
    "            pred_prob = model.predict(np.expand_dims(X_test[idx], axis=0), verbose=0)[0][0]\n",
    "            pred_class = \"Cat\" if pred_prob > 0.5 else \"Dog\"\n",
    "            \n",
    "            axes[i, j + 1].imshow(img)\n",
    "            color = 'green' if pred_class == true_class else 'red'\n",
    "            axes[i, j + 1].set_title(f'{name}\\nPred: {pred_class}\\nConf: {pred_prob:.3f}', \n",
    "                                   color=color, fontweight='bold')\n",
    "            axes[i, j + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59258939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VISUAL PREDICTION EXAMPLES\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Show examples from your best model (transfer learning model)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m show_prediction_examples(transfer_model, \u001b[43mX_test\u001b[49m, Y_test, \u001b[33m\"\u001b[39m\u001b[33mTransfer Learning Model\u001b[39m\u001b[33m\"\u001b[39m, num_examples=\u001b[32m8\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Show a single prediction (like your original code)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSingle Random Prediction:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUAL PREDICTION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show examples from your best model (transfer learning model)\n",
    "show_prediction_examples(transfer_model, X_test, Y_test, \"Transfer Learning Model\", num_examples=8)\n",
    "\n",
    "# Show a single prediction (like your original code)\n",
    "print(\"\\nSingle Random Prediction:\")\n",
    "show_single_prediction(transfer_model, X_test, Y_test, \"Transfer Learning Model\")\n",
    "\n",
    "# If you have both models, compare their predictions\n",
    "if 'improved_model' in locals() and 'transfer_model' in locals():\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    compare_model_predictions([improved_model, transfer_model], \n",
    "                            [\"Enhanced CNN\", \"Transfer Learning\"], \n",
    "                            X_test, Y_test, num_examples=4)\n",
    "\n",
    "# =============================================================================\n",
    "# ADDITIONAL: Show best and worst predictions\n",
    "# =============================================================================\n",
    "\n",
    "def show_best_worst_predictions(model, X_test, Y_test, model_name=\"Model\"):\n",
    "    \"\"\"Show the most confident correct and incorrect predictions\"\"\"\n",
    "    \n",
    "    # Get all predictions\n",
    "    all_preds = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate confidence for each prediction\n",
    "    confidences = np.abs(all_preds.flatten() - 0.5)  # Distance from 0.5 (uncertainty)\n",
    "    \n",
    "    # Get correct and incorrect predictions\n",
    "    pred_classes = (all_preds.flatten() > 0.5).astype(int)\n",
    "    true_classes = Y_test.flatten().astype(int)\n",
    "    correct_mask = pred_classes == true_classes\n",
    "    \n",
    "    # Find most confident correct predictions\n",
    "    correct_indices = np.where(correct_mask)[0]\n",
    "    correct_confidences = confidences[correct_indices]\n",
    "    most_confident_correct = correct_indices[np.argsort(correct_confidences)[-4:]]\n",
    "    \n",
    "    # Find most confident incorrect predictions\n",
    "    incorrect_indices = np.where(~correct_mask)[0]\n",
    "    if len(incorrect_indices) > 0:\n",
    "        incorrect_confidences = confidences[incorrect_indices]\n",
    "        most_confident_incorrect = incorrect_indices[np.argsort(incorrect_confidences)[-4:]]\n",
    "    else:\n",
    "        most_confident_incorrect = []\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Show most confident correct predictions\n",
    "    for i, idx in enumerate(most_confident_correct):\n",
    "        img = X_test[idx].copy()\n",
    "        if img.min() < 0:\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        axes[0, i].imshow(img)\n",
    "        true_class = \"Cat\" if Y_test[idx][0] > 0.5 else \"Dog\"\n",
    "        pred_prob = all_preds[idx][0]\n",
    "        pred_class = \"Cat\" if pred_prob > 0.5 else \"Dog\"\n",
    "        \n",
    "        axes[0, i].set_title(f'✅ CORRECT\\nTrue: {true_class}\\nConf: {pred_prob:.3f}', \n",
    "                           color='green', fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Show most confident incorrect predictions\n",
    "    for i in range(4):\n",
    "        if i < len(most_confident_incorrect):\n",
    "            idx = most_confident_incorrect[i]\n",
    "            img = X_test[idx].copy()\n",
    "            if img.min() < 0:\n",
    "                img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            axes[1, i].imshow(img)\n",
    "            true_class = \"Cat\" if Y_test[idx][0] > 0.5 else \"Dog\"\n",
    "            pred_prob = all_preds[idx][0]\n",
    "            pred_class = \"Cat\" if pred_prob > 0.5 else \"Dog\"\n",
    "            \n",
    "            axes[1, i].set_title(f'❌ INCORRECT\\nTrue: {true_class}\\nPred: {pred_class}\\nConf: {pred_prob:.3f}', \n",
    "                               color='red', fontweight='bold')\n",
    "            axes[1, i].axis('off')\n",
    "        else:\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Most Confident Predictions', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show best and worst predictions\n",
    "print(\"\\nMost Confident Predictions:\")\n",
    "show_best_worst_predictions(transfer_model, X_test, Y_test, \"Transfer Learning Model\")\n",
    "\n",
    "print(\"\\n🎉 Visual prediction analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4dacc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
